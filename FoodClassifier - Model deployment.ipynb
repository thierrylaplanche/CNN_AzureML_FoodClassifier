{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food Classifier Project\n",
    "#### Created by Thierry LAPLANCHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents :\n",
    "1. Import libraries\n",
    "2. Install Azure Machine Learning SDK for Python\n",
    "3. Create a Machine Learning workspace\n",
    "4. Connect to your ML workspace\n",
    "5. Register the model from TFSaved Model folder\n",
    "6. Create an inference configuration\n",
    "7. Deploy the model to Azure Container Instances\n",
    "8. Test the endpoint\n",
    "9. Update the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Azure ML libraries\n",
    "from azureml.core import Workspace\n",
    "import urllib.request\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Environment\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.webservice import AciWebservice, Webservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your Azure subscription ID in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = 'XXXXXXXXXXXXXXXXXXXXXX' # enter your own Azure subscription ID (https://portal.azure.com/#blade/Microsoft_Azure_Billing/SubscriptionsBlade)\n",
    "location='southeastasia' # location where to host the workspace and the model\n",
    "workspace_name='foodidentifierworkspace' # name of the workspace to create (will result in an error if name already exists)\n",
    "resourcegroup_name='foodidentifierresourcegroup' # name of the resource group (will be created automatically if non-existent)\n",
    "model_name='foodidentifiermodel' # name of the model to deploy\n",
    "service_name='foodidentifierservice' # name of the service once the model is deployed (can be the same name as the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Azure Machine Learning SDK for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the next cell if you need to install Azure SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install azureml-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a Machine Learning workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code only the first time, if you don't already have a Machine Learning workspace in your Azure environment.  \n",
    "Otherwise, go to step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.create(name=workspace_name,\n",
    "               subscription_id=subscription_id,\n",
    "               resource_group=resourcegroup_name,\n",
    "               create_resource_group=True,\n",
    "               location=location\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connect to your ML workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created an ML workspace, connect to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace(subscription_id=subscription_id,\n",
    "               resource_group=resourcegroup_name,\n",
    "               workspace_name=workspace_name)\n",
    "\n",
    "print(\"Workspace name: {}\".format(ws.name) +\n",
    "      \"\\nResource group: {}\".format(ws.resource_group) +\n",
    "      \"\\nLocation: {}\".format(ws.location) +\n",
    "      \"\\nSubscription ID: {}\".format(ws.subscription_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Register the model from TFSavedModel folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will register the model saved locally to your Azure ML workspace.  \n",
    "The model you have trained must be saved in the path defined in the following cell (by default : 'TFSavedModel')  \n",
    "This step may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model foodidentifiermodelbaseline\n",
      "Name: foodidentifiermodelbaseline\n",
      "Version: 1\n"
     ]
    }
   ],
   "source": [
    "# Register the model which is saved in the TFSavedModel folder\n",
    "model = Model.register(ws,\n",
    "                       model_name=model_name, # name of the model\n",
    "                       model_path='./TFSavedModel', # local path to the model files\n",
    "                       model_framework=Model.Framework.TENSORFLOW, # framework on which the model is based\n",
    "                       model_framework_version=tf.__version__, # version of the TensorFlow framework\n",
    "                       description='Model to identify food.') # description of the model\n",
    "\n",
    "print('Name:', model.name)\n",
    "print('Version:', model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create an inference configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure your inference environment so that the model can be called as a Web Service from an external application.  \n",
    "First, we will create a YAML configuration file which contains all the Python dependencies that are required to execute the model.  \n",
    "Those are the libraries which are used in the entry script of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'myenv.yml'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dependencies configuration file myenv.yml\n",
    "cd = CondaDependencies.create()\n",
    "cd.add_conda_package('numpy')\n",
    "cd.add_conda_package('pillow')\n",
    "cd.add_pip_package('tensorflow==2.3.0')\n",
    "cd.add_pip_package('pickle-mixin')\n",
    "cd.add_pip_package(\"azureml-contrib-services\")\n",
    "cd.add_pip_package(\"azureml-defaults\")\n",
    "cd.save_to_file(base_directory='./', conda_file_path='myenv.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the inference configuration and specifiy the location of the entry script.  \n",
    "You might need to modify the number of CPU cores or memory allocated to the execution of your model, according to its complexity and the speed of execution you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deployment configuration\n",
    "inference_env = Environment.from_conda_specification(name=\"inference_env\", file_path=\"myenv.yml\") # inference environment\n",
    "inference_config = InferenceConfig(entry_script=\"entry_script.py\", source_directory=\"./deployment\", environment=inference_env) # inference configuration which specifies the entry script\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, # number of CPU cores\n",
    "                                               memory_gb=1, # amount of memory\n",
    "                                               description='Model to identify food.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deploy the model to Azure Container Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will de deployed to an isolated container instance with a unique endpoint (URI) to which you will call your model.  \n",
    "This step may take over 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: ['foodidentifiermodelbaseline:1']\n",
      "Entry script: deployment\\entry_script.py\n",
      "Environment dependencies: ['python=3.6.2', ordereddict([('pip', ['tensorflow==2.3.0', 'pickle-mixin', 'azureml-contrib-services', 'azureml-defaults'])]), 'numpy', 'pillow']\n",
      "Environment docker image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210531.v1\n",
      "CPU requirement: 1, Memory requirement: 1GB\n",
      "Uploading dependency C:\\Users\\thier\\AppData\\Local\\Temp\\tmpds7_htwt\\10318f7f.tar.gz.\n",
      "Request submitted, please run wait_for_deployment(show_output=True) to get deployment status.\n",
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2021-07-13 18:29:22+09:00 Creating Container Registry if not exists.\n",
      "2021-07-13 18:29:22+09:00 Registering the environment.\n",
      "2021-07-13 18:29:23+09:00 Use the existing image.\n",
      "2021-07-13 18:29:24+09:00 Generating deployment configuration.\n",
      "2021-07-13 18:29:26+09:00 Submitting deployment to compute..\n",
      "2021-07-13 18:29:32+09:00 Checking the status of deployment foodidentifierservicebaseline..\n",
      "2021-07-13 18:30:08+09:00 Checking the status of inference endpoint foodidentifierservicebaseline.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n",
      "2021-07-13T09:30:03,571388100+00:00 - rsyslog/run \n",
      "2021-07-13T09:30:03,588735700+00:00 - gunicorn/run \n",
      "File not found: /var/azureml-app/.\n",
      "Starting HTTP server\n",
      "2021-07-13T09:30:03,579598200+00:00 - iot-server/run \n",
      "2021-07-13T09:30:03,624325700+00:00 - nginx/run \n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2021-07-13T09:30:03,975121600+00:00 - iot-server/finish 1 0\n",
      "2021-07-13T09:30:03,977288500+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 20.1.0\n",
      "Listening at: http://127.0.0.1:31311 (62)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 90\n",
      "2021-07-13 09:30:05.613537: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_0890de94156ff4f31d9b98e1c432dfbe/lib:/azureml-envs/azureml_0890de94156ff4f31d9b98e1c432dfbe/lib:\n",
      "2021-07-13 09:30:05.613621: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Initializing logger\n",
      "2021-07-13 09:30:08,778 | root | INFO | Starting up app insights client\n",
      "logging socket was found. logging is available.\n",
      "logging socket was found. logging is available.\n",
      "2021-07-13 09:30:08,779 | root | INFO | Starting up request id generator\n",
      "2021-07-13 09:30:08,779 | root | INFO | Starting up app insight hooks\n",
      "2021-07-13 09:30:08,779 | root | INFO | Invoking user's init function\n",
      "2021-07-13 09:30:09.576432: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_0890de94156ff4f31d9b98e1c432dfbe/lib:/azureml-envs/azureml_0890de94156ff4f31d9b98e1c432dfbe/lib:\n",
      "2021-07-13 09:30:09.576510: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-07-13 09:30:09.576546: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (SandboxHost-637617653762893598): /proc/driver/nvidia/version does not exist\n",
      "2021-07-13 09:30:09.577041: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-13 09:30:09.594707: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2294685000 Hz\n",
      "2021-07-13 09:30:09.595473: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ecc7270ec0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-07-13 09:30:09.595616: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-07-13 09:30:39,378 | root | INFO | Users's init has completed successfully\n",
      "2021-07-13 09:30:39,382 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2021-07-13 09:30:39,383 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2021-07-13 09:30:39,383 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2021-07-13 09:30:41,272 | root | INFO | Swagger file not present\n",
      "2021-07-13 09:30:41,273 | root | INFO | 404\n",
      "127.0.0.1 - - [13/Jul/2021:09:30:41 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-07-13 09:30:44,163 | root | INFO | Swagger file not present\n",
      "2021-07-13 09:30:44,163 | root | INFO | 404\n",
      "127.0.0.1 - - [13/Jul/2021:09:30:44 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deploy the model to Azure Container Instances\n",
    "service = Model.deploy(workspace=ws, \n",
    "                           name=service_name, \n",
    "                           models=[model], \n",
    "                           inference_config=inference_config, \n",
    "                           deployment_config=aciconfig,\n",
    "                           overwrite=True, # allows to overwrite an existing deployment\n",
    "                           show_output=True)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The URI of the endpoint is: http://be42e604-f4f7-46f8-819f-bb046c43338d.southeastasia.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "uri = service.scoring_uri\n",
    "\n",
    "print(\"The URI of the endpoint is: {}\".format(service.scoring_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a tool called HTTPie to call our web service with a picture as a parameter.  \n",
    "Uncomment the following line to install HTTPie tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --upgrade httpie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command to call the web service from Jupyter Notebook is:  \n",
    "!http --ignore-stdin -f POST http://uri_of_the_endpoint file@tile_of_the_picture.jpg  \n",
    "You can replace the name of the picture 'test1.png' with a picture of your choice (saved in the same folder as this file) in the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!http --ignore-stdin -f POST $uri file@test1.png > result.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web service returns JSON data. We saved it into 'result.json'.  \n",
    "Let's see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bak kuh teh pork rib soup': 0.012148265726864338,\n",
       " 'Baklava': 0.004917326848953962,\n",
       " 'Char kway teow': 2.9208351861598203e-06,\n",
       " 'Chilli crab': 2.0984433035664551e-07,\n",
       " 'Hummus': 0.9747801423072815,\n",
       " 'Kanafeh': 8.537168469047174e-05,\n",
       " 'Rojak': 6.1727905631414615e-06,\n",
       " 'Seafood hor fun': 1.6346919437637553e-05,\n",
       " 'Shawarma': 0.008019939064979553,\n",
       " 'Tabouleh': 2.322893669770565e-05}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open the results from the JSON file\n",
    "with open('result.json') as json_file:\n",
    "    prediction = json.load(json_file)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a dictionnary in which the keys are the classes (name of the dish) and the values are the probabilities.  \n",
    "Let's sort the results by reverse order so that the item with highest probability comes first in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hummus', 0.9747801423072815),\n",
       " ('Bak kuh teh pork rib soup', 0.012148265726864338),\n",
       " ('Shawarma', 0.008019939064979553),\n",
       " ('Baklava', 0.004917326848953962),\n",
       " ('Kanafeh', 8.537168469047174e-05),\n",
       " ('Tabouleh', 2.322893669770565e-05),\n",
       " ('Seafood hor fun', 1.6346919437637553e-05),\n",
       " ('Rojak', 6.1727905631414615e-06),\n",
       " ('Char kway teow', 2.9208351861598203e-06),\n",
       " ('Chilli crab', 2.0984433035664551e-07)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_prediction = [(k, prediction[k]) for k in sorted(prediction, key=prediction.get, reverse=True)]\n",
    "\n",
    "sorted_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top prediction for this picture is Hummus with a probability of 0.9747801423072815\n"
     ]
    }
   ],
   "source": [
    "print('The top prediction for this picture is {} with a probability of {}'.format(sorted_prediction[0][0],sorted_prediction[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Update the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you retrain your model, you need to register the new model to your Azure ML workspace and link it to the existing endpoint.  \n",
    "Again, your new model must be saved in the path specified in the following cell ('TFSavedModel' by default).  \n",
    "The version of the model will be increased by Azure.  \n",
    "This step may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model foodidentifiermodelbaseline\n",
      "Name: foodidentifiermodelbaseline\n",
      "Version: 3\n"
     ]
    }
   ],
   "source": [
    "# Register the updated model which is saved in the TFSavedModel folder\n",
    "new_model = Model.register(ws,\n",
    "                       model_name=\"foodidentifiermodelbaseline\", # name of the model\n",
    "                       model_path='./TFSavedModel', # local path to the model files\n",
    "                       model_framework=Model.Framework.TENSORFLOW, # framework on which the model is based\n",
    "                       model_framework_version=tf.__version__, # version of the TensorFlow framework\n",
    "                       description='Model to identify food.') # description of the model\n",
    "\n",
    "print('Name:', new_model.name)\n",
    "print('Version:', new_model.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the existing workspace\n",
    "ws = Workspace(subscription_id=subscription_id,\n",
    "               resource_group=resourcegroup_name,\n",
    "               workspace_name=workspace_name)\n",
    "\n",
    "print(\"Workspace name: {}\".format(ws.name) +\n",
    "      \"\\nResource group: {}\".format(ws.resource_group) +\n",
    "      \"\\nLocation: {}\".format(ws.location) +\n",
    "      \"\\nSubscription ID: {}\".format(ws.subscription_id))\n",
    "\n",
    "# Create a deployment configuration\n",
    "inference_env = Environment.from_conda_specification(name=\"inference_env\", file_path=\"myenv.yml\") # inference environment\n",
    "inference_config = InferenceConfig(entry_script=\"entry_script.py\", source_directory=\"./deployment\", environment=inference_env) # inference configuration which specifies the entry script\n",
    "\n",
    "# Retrieve the existing service (endpoint)\n",
    "service = Webservice(name=\"foodidentifierservicebaseline\", workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2021-07-13 21:44:53+09:00 Creating Container Registry if not exists.\n",
      "2021-07-13 21:44:53+09:00 Registering the environment.\n",
      "2021-07-13 21:44:54+09:00 Use the existing image.\n",
      "2021-07-13 21:44:55+09:00 Generating deployment configuration.\n",
      "2021-07-13 21:44:56+09:00 Submitting deployment to compute.\n",
      "2021-07-13 21:44:58+09:00 Checking the status of deployment foodidentifierservicebaseline..\n",
      "2021-07-13 21:55:07+09:00 Checking the status of inference endpoint foodidentifierservicebaseline.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n",
      "2021-07-13T12:54:32,668242600+00:00 - rsyslog/run \n",
      "2021-07-13T12:54:32,672586700+00:00 - gunicorn/run \n",
      "File not found: /var/azureml-app/.\n",
      "Starting HTTP server\n",
      "2021-07-13T12:54:32,683411100+00:00 - iot-server/run \n",
      "2021-07-13T12:54:32,693424700+00:00 - nginx/run \n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2021-07-13T12:54:33,100374500+00:00 - iot-server/finish 1 0\n",
      "2021-07-13T12:54:33,102931900+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 20.1.0\n",
      "Listening at: http://127.0.0.1:31311 (68)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 95\n",
      "2021-07-13 12:54:35.061733: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_0890de94156ff4f31d9b98e1c432dfbe/lib:/azureml-envs/azureml_0890de94156ff4f31d9b98e1c432dfbe/lib:\n",
      "2021-07-13 12:54:35.061820: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Initializing logger\n",
      "2021-07-13 12:54:38,778 | root | INFO | Starting up app insights client\n",
      "logging socket was found. logging is available.\n",
      "logging socket was found. logging is available.\n",
      "2021-07-13 12:54:38,783 | root | INFO | Starting up request id generator\n",
      "2021-07-13 12:54:38,783 | root | INFO | Starting up app insight hooks\n",
      "2021-07-13 12:54:38,783 | root | INFO | Invoking user's init function\n",
      "2021-07-13 12:54:39.727692: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_0890de94156ff4f31d9b98e1c432dfbe/lib:/azureml-envs/azureml_0890de94156ff4f31d9b98e1c432dfbe/lib:\n",
      "2021-07-13 12:54:39.727810: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-07-13 12:54:39.727850: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (SandboxHost-637617771086753759): /proc/driver/nvidia/version does not exist\n",
      "2021-07-13 12:54:39.731509: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-13 12:54:39.755600: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2294685000 Hz\n",
      "2021-07-13 12:54:39.756299: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56488b2f56e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-07-13 12:54:39.756489: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-07-13 12:55:14,998 | root | INFO | Users's init has completed successfully\n",
      "2021-07-13 12:55:15,005 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2021-07-13 12:55:15,005 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2021-07-13 12:55:15,007 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2021-07-13 12:55:15,025 | root | INFO | Swagger file not present\n",
      "2021-07-13 12:55:15,027 | root | INFO | 404\n",
      "127.0.0.1 - - [13/Jul/2021:12:55:15 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-07-13 12:55:17,850 | root | INFO | Swagger file not present\n",
      "2021-07-13 12:55:17,851 | root | INFO | 404\n",
      "127.0.0.1 - - [13/Jul/2021:12:55:17 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update the existing endpoint with the new model (this step may take a few minutes)\n",
    "service.update(models=[new_model], inference_config=inference_config)\n",
    "service.wait_for_deployment(show_output=True)\n",
    "\n",
    "print(service.state)\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
